---
title: "8.2 Exercise"
author: "Harlan Wittlieff"
date: "10/31/2021"
output: pdf_document
---

# Assignment 06

## Load the `data/r4ds/heights.csv` to
```{r include = TRUE}
heights_df <- read.csv("data/r4ds/heights.csv")
```
## Load the ggplot2 library
```{r include = TRUE}
library(ggplot2)
```
## Fit a linear model using the `age` variable as the predictor and `earn` as the outcome
```{r include = TRUE}
age_lm <- lm(earn ~ age, data = heights_df)
```
## View the summary of your model using `summary()`
```{r include = TRUE}
summary(age_lm)
```
## Creating predictions using `predict()`
```{r include = TRUE}
age_predict_df <- data.frame(earn = predict(age_lm, heights_df), age=heights_df$age)
```
## Plot the predictions against the original data
```{r echo = FALSE}
ggplot(data = heights_df, aes(y = earn, x = age)) +
  geom_point(color='blue') +
  geom_line(color='red',data = age_predict_df, aes(y=earn, x=age))
```
## Calculated values
```{r include = TRUE}
mean_earn <- mean(heights_df$earn)
## Corrected Sum of Squares Total
sst <- sum((mean_earn - heights_df$earn)^2)
## Corrected Sum of Squares for Model
ssm <- sum((mean_earn - age_predict_df$earn)^2)
## Residuals
residuals <- heights_df$earn - age_predict_df$earn
## Sum of Squares for Error
sse <- sum(residuals^2)
## R Squared R^2 = SSM\SST
r_squared <- ssm/sst
r_squared
## Number of observations
n <- nrow(heights_df)
n
## Number of regression parameters
p <- 2
## Corrected Degrees of Freedom for Model (p-1)
dfm <- p-1
dfm
## Degrees of Freedom for Error (n-p)
dfe <- n-p
dfe
## Corrected Degrees of Freedom Total:   DFT = n - 1
dft <- n-1
dft
## Mean of Squares for Model:   MSM = SSM / DFM
msm <- ssm/dfm
msm
## Mean of Squares for Error:   MSE = SSE / DFE
mse <- sse/dfe
mse
## Mean of Squares Total:   MST = SST / DFT
mst <- sst/dfe
mst
## F Statistic F = MSM/MSE
f_score <- msm/mse
f_score
## Adjusted R Squared R2 = 1 - (1 - R2)(n - 1) / (n - p)
adjusted_r_squared <- 1-(1-r_squared)*(n-1)/(n-p)
adjusted_r_squared
## Calculate the p-value from the F distribution
p_value <- pf(f_score, dfm, dft, lower.tail=F)
p_value
```

# Assignment 07

## Load the `data/r4ds/heights.csv` to
```{r include = TRUE}
heights_df <- read.csv("data/r4ds/heights.csv")
```
# Fit a linear model
```{r include = TRUE}
earn_lm <-  lm(earn ~ height + sex + ed + age + race, data=heights_df)
```
# View the summary of your model
```{r include = TRUE}
summary(earn_lm)
```
## Build a df of predicted values
```{r include = TRUE}
predicted_df <- data.frame(
  earn = predict(earn_lm, heights_df),
  ed=heights_df$ed, race=heights_df$race, height=heights_df$height,
  age=heights_df$age, sex=heights_df$sex
  )
```
## Calculated Values
```{r include = TRUE}
## Compute deviation (i.e. residuals)
mean_earn <- mean(heights_df$earn)
## Corrected Sum of Squares Total
sst <- sum((mean_earn - heights_df$earn)^2)
## Corrected Sum of Squares for Model
ssm <- sum((mean_earn - predicted_df$earn)^2)
## Residuals
residuals <- heights_df$earn - predicted_df$earn
## Sum of Squares for Error
sse <- sum(residuals^2)
## R Squared
r_squared <- ssm/sst
r_squared
## Number of observations
n <- nrow(heights_df)
## Number of regression paramaters
p <- 8
## Corrected Degrees of Freedom for Model
dfm <- p-1
## Degrees of Freedom for Error
dfe <- n-p
## Corrected Degrees of Freedom Total:   DFT = n - 1
dft <- n-1
## Mean of Squares for Model:   MSM = SSM / DFM
msm <- ssm/dfm
## Mean of Squares for Error:   MSE = SSE / DFE
mse <- sse/dfe
## Mean of Squares Total:   MST = SST / DFT
mst <- sst/dfe
## F Statistic
f_score <- msm/mse
f_score
## Adjusted R Squared R2 = 1 - (1 - R2)(n - 1) / (n - p)
adjusted_r_squared <- 1-(1-r_squared)*(n-1)/(n-p)
adjusted_r_squared
```

# Housing Data
```{r include=FALSE}
library('readxl')
library('stringr')
housing_df <- read_excel("data/week-7-housing.xlsx")
names(housing_df) <- str_replace_all(names(housing_df), c(" " = "_"))
housing_df$year_renovated[housing_df$year_renovated == 0] <- NA
housing_df$bathroom_count <- 1*housing_df$bath_full_count + .75*housing_df$bath_3qtr_count + .5*housing_df$bath_half_count
```
## Explain any transformations or modifications you made to the dataset
* Replaced spaces in column names with "_"
* Replaced any values of 0 in year_renovated with NA
* Created a new bathroom_count column based on the number of full and partial bathrooms

## Create a model to predict sale price based on square foot of lot
```{r include=TRUE}
salesprice_lm <-  lm(Sale_Price ~ sq_ft_lot, data=housing_df)
```
## Create a model that predicts sale price based on square foot of lot and several other additional predictors
Additional predictors used
* building_grade
* square_feet_total_living
* bedrooms
* custom "bathroom_count" field
These fields were chosen due to their historical correlation with sales price in other markets
```{r include=TRUE}
salesprice2_lm <-  lm(Sale_Price ~ sq_ft_lot + building_grade + square_feet_total_living + bedrooms + bathroom_count, data=housing_df)
```

## Execute a summary() function on two variables defined in the previous step to compare the model results. What are the R2 and Adjusted R2 statistics? Explain what these results tell you about the overall model. Did the inclusion of the additional predictors help explain any large variations found in Sale Price?

```{r include=TRUE}
summary(salesprice_lm)
```

```{r include=TRUE}
summary(salesprice2_lm)
```
From the original model, the R2 values is 0.01435 and the adjusted R2 value is 0.01428. This means that not much predictive power is lost due to shrinkage, however the model only can account for 1.4% of the variation in sale price. 

The second model has an R2 value of .2148 and a adjusted R2 value of 0.2145. This model also does not lose much predictive power due to shrinkage. The inclusion of the additional predictors increased the predictability of the model when compared to the original.

## Considering the parameters of the multiple regression model you have created. What are the standardized betas for each parameter and what do the values indicate?

```{r include = TRUE}
library('QuantPsyc')
lm.beta(salesprice2_lm)
```
* sq_ft_lot
    + as sq_ft_lot increases by 1 standard deviation, sales prices increases by 0.018 standard deviations
* building_grade
    + as building_grade increases by 1 standard deviation, sales prices increases by 0.108 standard deviations  
* square_feet_total_living
    + as square_feet_total_living increases by 1 standard deviation, sales prices increases by 0.364 standard deviations  
* bedrooms
    + as bedrooms increases by 1 standard deviation, sales prices decreases by 0.043 standard deviations  
* bathroom_count
    + as bathroom_count increases by 1 standard deviation, sales prices increases by 0.041 standard deviations 

## Calculate the confidence intervals for the parameters in your model and explain what the results indicate.
```{r include = TRUE}
confint(salesprice2_lm)
```
square_feet_total_living has a tight confidence interval indicating that the true value is likely represented in the model. None of the other variables cross 0, indicating that they may be less representative, but still significant. 

## Assess the improvement of the new model compared to your original model (simple regression model) by testing whether this change is significant by performing an analysis of variance.
```{r include = TRUE}
anova(salesprice_lm, salesprice2_lm)
```
The value of Pr(>F) is equal to 2.2e-16. salesprice2_lm significantly improved the fit of the model to the data compared to salesprice_lm. 

## Perform casewise diagnostics to identify outliers and/or influential cases, storing each function's output in a dataframe assigned to a unique variable name.

### Calculate the standardized residuals using the appropriate command, specifying those that are +-2, storing the results of large residuals in a variable you create.

```{r include=TRUE}
housing_df$standardized.residuals <- rstandard(salesprice2_lm)
housing_df$large.residual <- housing_df$standardized.residuals>2 | housing_df$standardized.residuals< -2
```

### Use the appropriate function to show the sum of large residuals.
```{r include=TRUE}
sum(housing_df$large.residual)
```
### Which specific variables have large residuals (only cases that evaluate as TRUE)?
```{r include=TRUE}
housing_df[housing_df$large.residual, c("addr_full", "Sale_Price", "sq_ft_lot", "building_grade", "square_feet_total_living", "bedrooms", "bathroom_count", "standardized.residuals" )]
```

### Investigate further by calculating the leverage, cooks distance, and covariance rations. Comment on all cases that are problematics.
```{r include=TRUE}
housing_df$leverage <- hatvalues(salesprice2_lm)
housing_df$cooks.distance <- cooks.distance(salesprice2_lm)
housing_df$covariance.ratios <- covratio(salesprice2_lm)
potential_problems_df <- housing_df[housing_df$large.residual, c("addr_full", "cooks.distance", "leverage", "covariance.ratios" )]
average_leverage <- (5 + 1)/nrow(housing_df)
problem_leverage <- average_leverage*3
cvr_high <- 1+(3*(5+1)/nrow(housing_df))
cvr_low <- 1-(3*(5+1)/nrow(housing_df))
potential_problems_df$problems <- potential_problems_df$cooks.distance>1 | potential_problems_df$leverage>problem_leverage | potential_problems_df$covariance.ratios>cvr_high | potential_problems_df$covariance.ratios<cvr_low
potential_problems_df[potential_problems_df$problems, c("addr_full", "cooks.distance", "leverage", "covariance.ratios")]

```
The listed values fail the tests for either cooks distance, leverage, or covariance ratios.

### Perform the necessary calculations to assess the assumption of independence and state if the condition is met or not.
```{r include = TRUE}
library('car')
dwt(salesprice2_lm)
```
The D-W Statistic is less than 1, meaning the model fails the test of independence.

### Perform the necessary calculations to assess the assumption of no multicollinearity and state if the condition is met or not.

```{r include = TRUE}
mean(vif(salesprice2_lm))
```
The result of 2.3 indicates that the regression may be biased.

### Visually check the assumptions related to the residuals using the plot() and hist() functions. Summarize what each graph is informing you of and if any anomalies are present.
```{r echo = TRUE}
plot(salesprice2_lm)
hist(rstudent(salesprice2_lm))
```
The plots show the potential for non-linearity and heteroscedasticity in the model.

### Overall, is this regression model unbiased? If an unbiased regression model, what does this tell us about the sample vs. the entire population model?

Overall, this regression model is biased. 


